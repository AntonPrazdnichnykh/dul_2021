{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework11_selfsupervised2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonPrazdnichnykh/dul_2021/blob/hw11/Homework/hw11/Homework11_selfsupervised2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
        "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
        "!pip install ./dul_2021"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRz5SIFKfZD",
        "outputId": "76ff4c57-d813-4a5b-9208-cc37d4aa0070"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dul_2021'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (198/198), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 361 (delta 112), reused 91 (delta 65), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (361/361), 55.77 MiB | 20.61 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n",
            "Processing ./dul_2021\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: dul-2021\n",
            "  Building wheel for dul-2021 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=26856 sha256=1ccbbfbe6c90db9684f799773c55c1cab2604fc03845d76d40c2ceae0d0a4fdd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e20edn8z/wheels/55/59/29/0fb1c635652157734f4d741f32fc11979149684e83e919de06\n",
            "Successfully built dul-2021\n",
            "Installing collected packages: dul-2021\n",
            "Successfully installed dul-2021-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dul_2021.utils.hw11_utils import *"
      ],
      "metadata": {
        "id": "KjY-iIy5MSZb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1. BYOL\n",
        "\n",
        "Here we will implement [BYOL](https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "* You can combine view, representation, and projection into one network. You can use same architechure as in practice. \n",
        "\n",
        "* Use BatchNorm\n",
        "\n",
        "* As predictor use few linear layers\n",
        "\n",
        "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "* τ = 0.99 (target update coefficient)\n",
        "* lr = 1e-4\n",
        "* num_epochs = 20\n",
        "* latent dim = 128\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "t7J5FgOHW6Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from tqdm import trange\n",
        "from itertools import chain"
      ],
      "metadata": {
        "id": "_9X5jiKJ4tA1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "QKiF4AfLCbtV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrast_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.RandomResizedCrop(size=28),\n",
        "                                          transforms.Normalize((0.5,), (0.5,))\n",
        "                                         ])"
      ],
      "metadata": {
        "id": "PWIbmT-C5dGN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, out_dim=128, hid_dim_full=128):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1, stride=2)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1, stride=2)\n",
        "        self.conv5 = nn.Conv2d(32, 32, 1)\n",
        "        self.conv6 = nn.Conv2d(32, 4, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.bn6 = nn.BatchNorm2d(4)\n",
        "\n",
        "        self.conv_to_fc = 7 * 7 * 4\n",
        "        self.fc1 = nn.Linear(self.conv_to_fc, hid_dim_full)\n",
        "        self.fc2 = nn.Linear(hid_dim_full, int(hid_dim_full // 2))\n",
        "\n",
        "        self.features = nn.Linear(int(hid_dim_full // 2), out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "\n",
        "        x = x.view(-1, self.conv_to_fc)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        features = self.features(x)\n",
        "\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "VLI_aVQb4c-F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data, transforms, n_views=2):\n",
        "    self.data = data\n",
        "    self.transforms = transforms\n",
        "    self.n_views = n_views\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.data[idx][0]\n",
        "    return torch.cat([self.transforms(sample) for _ in range(self.n_views)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "9b08WZBATn9h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BYOL(nn.Module):\n",
        "  def __init__(self, latent_dim=128, hidden_dim=128, tau=0.99):\n",
        "    super().__init__()\n",
        "    self.enc = Net(latent_dim, hidden_dim)\n",
        "    self.mom_enc = Net(latent_dim, hidden_dim)\n",
        "    for p1, p2 in zip(self.enc.parameters(), self.mom_enc.parameters()):\n",
        "      p2.data.copy_(p1.data)\n",
        "      p2.requires_grad = False\n",
        "\n",
        "    self.predictor = nn.Sequential(\n",
        "        nn.Linear(latent_dim, 2 * latent_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(2 * latent_dim, latent_dim)\n",
        "    )\n",
        "    self.tau = tau\n",
        "\n",
        "  def update_momentum_weights(self):\n",
        "    for p1, p2 in zip(self.enc.parameters(), self.mom_enc.parameters()):\n",
        "      p2.data.copy_(self.tau * p2.data + (1 - self.tau) * p1.data)\n",
        "\n",
        "  def loss(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x_stud, x_teach = x.chunk(2, dim=1)\n",
        "    feats_orig = self.predictor(self.enc(x_stud))\n",
        "    feats_contr = self.mom_enc(x_teach)\n",
        "    loss = 1 - F.cosine_similarity(feats_orig, feats_contr)\n",
        "    return loss.mean()\n",
        "\n",
        "  def fit(self, train_data, batch_size=512, lr=1e-4, num_epochs=20):\n",
        "    train_data = MyDataset(train_data, transforms=contrast_transforms)\n",
        "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = Adam(chain(self.enc.parameters(), self.predictor.parameters()), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in trange(num_epochs, desc='Training'):\n",
        "      for batch in dataloader:\n",
        "\n",
        "        loss = self.loss(batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        self.update_momentum_weights()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return np.array(losses)\n",
        "\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def encode(self, batch):\n",
        "    self.eval()\n",
        "    out = self.enc(batch.to(device))\n",
        "    self.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "Xh3uTWPB5z1h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q1(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\"\n",
        "    model = BYOL()\n",
        "    model.to(device)\n",
        "    losses = model.fit(train_data)\n",
        "    return losses, model.encode\n"
      ],
      "metadata": {
        "id": "Og9Fv7sV6nrO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "WBWwQajZdend",
        "outputId": "cf83268c-2b2c-4630-b719-cdf3a26d3a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
        "q1_results(q1, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "3Ib0ne2mX80s",
        "outputId": "4df620f2-7f48-48c3-b456-3d57e129b597"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 20/20 [20:50<00:00, 62.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean classification accuracy=0.3931\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2. Barlow Twins\n",
        "\n",
        "Here we will implement [barlow twins](https://arxiv.org/abs/2103.03230).\n",
        "\n",
        "* You can use same architechure as in practice. \n",
        "\n",
        "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "* λ = 0.01 \n",
        "* lr = 5e-4\n",
        "* num_epochs = 20\n",
        "* latent dim = 128\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "hlbaIthyMGab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q2(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "pd6RrZfP75HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
        "q2_results(q2, True)"
      ],
      "metadata": {
        "id": "EA1Z_s1a8_sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus. SwAV\n",
        "\n",
        "Here we will implement [SwAV](https://arxiv.org/abs/2006.09882v5)\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "D8UN9nr9aYGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def b(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "1Yin_8Ebaa8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_results(b, True)"
      ],
      "metadata": {
        "id": "S0jgIgLN8tzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}